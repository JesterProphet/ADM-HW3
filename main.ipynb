{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed44c60",
   "metadata": {},
   "source": [
    "# Homework 3 - Master's Degrees from all over!\n",
    "\n",
    "#### Group 2 <br>\n",
    "\n",
    "<div style=\"float: left;\">\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Student</th>\n",
    "            <th>GitHub</th>\n",
    "            <th>Matricola</th>\n",
    "            <th>E-Mail</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Andr√© Leibrant</td>\n",
    "            <td>JesterProphet</td>\n",
    "            <td>2085698</td>\n",
    "            <td>leibrant.2085698@studenti.uniroma1.it</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Gloria Kim</td>\n",
    "            <td>keemgloria</td>\n",
    "            <td>1862339</td>\n",
    "            <td>kim.1862339@studenti.uniroma1.it</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a14032",
   "metadata": {},
   "source": [
    "#### Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f468104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "import geopandas as gpd\n",
    "import googlemaps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from keplergl import KeplerGl\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9500839",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "### 1.1. Get the list of master's degree courses\n",
    "\n",
    "We start with the list of courses to include in your corpus of documents. In particular, we focus on web scrapping the [MSc Degrees](https://github.com/Sapienza-University-Rome/ADM/tree/master/2023/Homework_3#:~:text=web%20scrapping%20the-,MSc%20Degrees,-.%20Next%2C%20we%20want). Next, we want you to collect the URL associated with each site in the list from the previously collected list. The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in the first 400 pages (each page has 15 courses, so you will end up with 6000 unique master's degree URLs).\n",
    "\n",
    "The output of this step is a .txt file whose single line corresponds to the master's URL.\n",
    "\n",
    "---\n",
    "\n",
    "For this we create a text file `links.txt` with all course links from every page. For this we take the core part of each page URL `https://www.findamasters.com/masters-degrees/msc-degrees/?PG=` and add the page number of each iteration at the end of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a86432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the text file if exists\n",
    "if os.path.exists(\"links.txt\"):\n",
    "    os.remove(\"links.txt\")\n",
    "\n",
    "# Create and open text file\n",
    "file = open(\"links.txt\", \"a\")\n",
    "\n",
    "# This is the main url of the website\n",
    "main_url = \"https://www.findamasters.com\"\n",
    "\n",
    "# This is the core part of each page url\n",
    "page_url = \"https://www.findamasters.com/masters-degrees/msc-degrees/?PG=\"\n",
    "\n",
    "# Parse through every page and collect all urls for each Master program\n",
    "for i in range(1, pages+1):\n",
    "    \n",
    "    # Define url for current page\n",
    "    url = f\"{page_url}{i}\"\n",
    "    \n",
    "    # Make a request to the current page\n",
    "    response = requests.get(url)\n",
    "   \n",
    "    # Get HTML from the response\n",
    "    html = response.text\n",
    "   \n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # Find all course links\n",
    "    links = soup.find_all(attrs={\"class\": \"courseLink text-dark\"})\n",
    "    \n",
    "    # Save all links in the text file\n",
    "    for link in links:\n",
    "    \n",
    "        # Save if link exists\n",
    "        if link[\"href\"]:\n",
    "            file.write(f\"{i}, {main_url}{link['href']}\\n\")\n",
    "        else:\n",
    "            print(link[\"href\"])\n",
    "\n",
    "# Close file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c8c61",
   "metadata": {},
   "source": [
    "### 1.2. Crawl master's degree pages\n",
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "1. Download the HTML corresponding to each of the collected URLs.\n",
    "2. After you collect a single page, immediately save its HTML in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "3. Organize the downloaded HTML pages into folders. Each folder will contain the HTML of the courses on page 1, page 2, ... of the list of master's programs.\n",
    "\n",
    "**Tip:** Due to the large number of pages you should download, you can use some methods that can help you shorten the time. If you employed a particular process or approach, kindly describe it.\n",
    "\n",
    "---\n",
    "\n",
    "First we created for all 400 pages one folder each inside the folder `pages` which is being created inside the project folder if it doesn't exist already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b0fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how many pages we want to parse\n",
    "pages = 400\n",
    "\n",
    "# Create folder pages inside the project\n",
    "os.makedirs(f\"pages\", exist_ok=True)\n",
    "\n",
    "# Create a folder for each page\n",
    "for i in range(1, pages+1):\n",
    "    \n",
    "    # Fill the page number with leading zeros\n",
    "    os.makedirs(f\"pages/page_{str(i).zfill(3)}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c347bf",
   "metadata": {},
   "source": [
    "In the next step we created a module `crawler.py` that parses through every URL link inside the text file `links.txt` and downloads the HTML content of the URL. We decided to use the library `requests` and created in addition a list with different user agents using different instances and web browser from which one is randomly selected for each iteration. This way we try to prevent that the website timeouts us. We also check if the page is still up to date and doesn't return the message `FindAMasters Page Not Found`. If yes, we insert inside the file we save the text `Page Not Found`. In case the website doesn't fully load the content of the URL for any reason we save the URL inside a text file `failed_files.txt`. After we go through every URL of `links.txt` we repeat the procedure for the failed links inside `failed_files.txt` until the HTML content of every URL was downloaded (meaning the file `failed_files.txt` is empty).\n",
    "\n",
    "To fasten up the running time we used the package `multiprocessing` and ran $n$ parallel processes where $n$ equals the number of kernels of the current system. In addition, we tried using different proxy addresses for every process which didn't improve our running time. So, we sticked to the solution using only the package `multiprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5241b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "subprocess.run([\"python\", \"crawler.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e816e3f",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n",
    "At this point, you should have all the HTML documents about the master's degree of interest, and you can start to extract specific information. The list of the information we desire for each course and their format is as follows:\n",
    "\n",
    "1. Course Name (to save as `courseName`): string;\n",
    "2. University (to save as `universityName`): string;\n",
    "3. Faculty (to save as `facultyName`): string\n",
    "4. Full or Part Time (to save as `isItFullTime`): string;\n",
    "5. Short Description (to save as `description`): string;\n",
    "6. Start Date (to save as `startDate`): string;\n",
    "7. Fees (to save as `fees`): string;\n",
    "8. Modality (to save as `modality`):string;\n",
    "9. Duration (to save as `duration`):string;\n",
    "10. City (to save as `city`): string;\n",
    "11. Country (to save as `country`): string;\n",
    "12. Presence or online modality (to save as `administration`): string;\n",
    "13. Link to the page (to save as `url`): string.\n",
    "\n",
    "---\n",
    "\n",
    "We created a module `parser.py` which goes throw the HTML content of every downloaded URL inside the `pages` folder and retrieves the information of interest for every course and saves the result inside a file for every course each inside the folder `courses` (the module creates the folder inside the project if it doesn't exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae6b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "subprocess.run([\"python\", \"parser.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfbe8d",
   "metadata": {},
   "source": [
    "## 2. Search Engine\n",
    "Now, we want to create two different Search Engines that, given as input a query, return the courses that match the query.\n",
    "\n",
    "### 2.0. Preprocessing\n",
    "#### 2.0.0) Preprocessing the text\n",
    "First, you must pre-process all the information collected for each MSc by:\n",
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed\n",
    "\n",
    "#### 2.0.1) Preprocessing the fees column\n",
    "Moreover, we want the field `fees` to collect numeric information. As you will see, you scraped textual information for this attribute in the dataset: sketch whatever method you need (using regex, for example, to find currency symbol) to collect information and, in case of multiple information, retrieve only the highest fees. Finally, once you have collected numerical information, you likely will have different currencies: this can be chaotic, so let chatGPT guide you in the choice and deployment of an API to convert this column to a common currency of your choice (it can be USD, EUR or whatever you want). Ultimately, you will have a float column renamed `fees (CHOSEN COMMON CURRENCY)`.\n",
    "\n",
    "---\n",
    "\n",
    "We created a module `preprocess.py` which includes a function `preprocess_text` that takes a text as a string, removes stopwords and punctuation, checks if it only contains alphabetical letters, and applies stemming (this function is being used throughout the other problems, too). For saving some computation time we preprocessed the `description` field and added the results inside the course files as the new column `preprocessed_description`.\n",
    "\n",
    "In addition, we preprocessed the `fees` field in the following way:\n",
    "\n",
    "1. Exclude all predefined cases `no_fees_keywords`\n",
    "2. Exclude field if it is a link using the function `is_valid_link`\n",
    "3. Retrieve fee in EUR with the function `get_fee`\n",
    "\n",
    "The function `get_fee` takes a string and extracts the (maximum) fee in EUR. First we retrieve with regex all numbers inside the `fees` field. We exclude fees between 1-50 because we treat those cases as outliers and also if the retrieved number is a year. If any number was found we try to retrieve all currencies using regex. In case we don't find any currency but the field includes the string `UK Fees:` we treat the retrieved fee as GBP. If we don't find any currency or we retrieve multiple currencies we exclude those cases because we don't have enough information to retrieve the correct fee. Otherwise we convert every fee regarding the predefined exchange course in `exchange_rates` and choose the maxium fee in case we have multiple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "subprocess.run([\"python\", \"preprocess.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f43dfdc",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query\n",
    "For the first version of the search engine, we narrowed our interest to the `description` of each course. It means that you will evaluate queries only concerning the course's description.\n",
    "\n",
    "#### 2.1.1) Create your index!\n",
    "Before building the index,\n",
    "\n",
    "Create a file named `vocabulary`, in the format you prefer, that maps each word to an integer (`term_id`).\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}\n",
    "```\n",
    "\n",
    "where `document_i` is the id of a document that contains that specific word.\n",
    "\n",
    "---\n",
    "\n",
    "Inside the module `engine` we create the function `create_inverted_index1` which creates an inverted index based on the `preprocessed_description` field and saves the vocabulary inside a pickle file `vocabulary_preprocessed_description_score1.pkl` inside the `vocabularies` folder (the module creates the folder inside the project if it doesn't exist). The function parses through every course file, skips the it if the field is empty or doesn't exist, and adds with the function `add_document` every word and corresponding course id to the vocabulary if they don't exist yet or just adds the course id to the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b04cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "engine.create_inverted_index1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aba943",
   "metadata": {},
   "source": [
    "#### 2.1.2) Execute the query\n",
    "Given a query input by the user, for example:\n",
    "\n",
    "```\n",
    "advanced knowledge\n",
    "````\n",
    "\n",
    "The Search Engine is supposed to return a list of documents.\n",
    "\n",
    "**What documents do we want?**<br>\n",
    "Since we are dealing with conjunctive queries (AND), each returned document should contain all the words in the query. The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "- `courseName`\n",
    "- `universityName`\n",
    "- `description`\n",
    "- `URL`\n",
    "\n",
    "---\n",
    "\n",
    "For this we created the function `conjunctive_query` which takes a query string and returns a pandas dataframe of all courses where every word of the query is inside the course description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"advanced knowledge\"\n",
    "engine.conjunctive_query(query).iloc[:, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c79abd",
   "metadata": {},
   "source": [
    "### 2.2. Conjunctive query & Ranking score\n",
    "For the second search engine, given a query, we want to get the top-*k* (the choice of *k* it's up to you!) documents related to the query. In particular:\n",
    "\n",
    "- Find all the documents that contain all the words in the query.\n",
    "- Sort them by their similarity with the query.\n",
    "- Return in output *k* documents, or all the documents with non-zero similarity with the query when the results are less than *k*. You must use a heap data structure (you can use Python libraries) for maintaining the top-*k* documents.\n",
    "\n",
    "To solve this task, you must use the *tfIdf* score and the *cosine similarity*. The field to consider is still the `description`. Let's see how.\n",
    "\n",
    "#### 2.2.1) Inverted index\n",
    "Your second Inverted Index must be of this format:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[(document1, tfIdf_{term,document1}), (document2, tfIdf_{term,document2}), (document4, tfIdf_{term,document4}), ...],\n",
    "term_id_2:[(document1, tfIdf_{term,document1}), (document3, tfIdf_{term,document3}), (document5, tfIdf_{term,document5}), (document6, tfIdf_{term,document6}), ...],\n",
    "...}\n",
    "```\n",
    "\n",
    "Practically, for each word, you want the list of documents in which it is contained and the relative *tfIdf* score.\n",
    "\n",
    "---\n",
    "\n",
    "For this we implemented a second function to create an inverted index `create_inverted_index2` which creates an inverted index based on the given column name using the *tfIdf* score and saves it inside a pickle file. If the column is not `preprocessed_description` we preprocess the field first. After that we create the *tfidf* matrix and based on this we create the inverted index keeping only the courses with a score larger than 0. The results are being saved inside `vocabulary_{column_name}.pkl`.\n",
    "\n",
    "In the following cell we create the inverted index using the *tfIdf* score for the field `preprocessed_description`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "engine.create_inverted_index2(\"preprocessed_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79961b86",
   "metadata": {},
   "source": [
    "#### 2.2.2) Execute the query\n",
    "In this new setting, given a query, you get the proper documents (i.e., those containing all the query's words) and sort them according to their similarity to the query. For this purpose, as the scoring function, we will use the *cosine similarity* concerning the *tfIdf* representations of the documents.\n",
    "\n",
    "Given a query input by the user, for example:\n",
    "\n",
    "```\n",
    "advanced knowledge\n",
    "````\n",
    "\n",
    "The search engine is supposed to return a list of documents, ranked by their *cosine similarity* to the query entered in the input.\n",
    "\n",
    "More precisely, the output must contain:\n",
    "\n",
    "- `courseName`\n",
    "- `universityName`\n",
    "- `description`\n",
    "- `URL`\n",
    "- The similarity score of the documents with respect to the query (float value between 0 and 1)\n",
    "\n",
    "---\n",
    "\n",
    "For this we created a function `retrieve_courses` which takes a query string and returns a pandas dataframe of the *k* (if no *k* is given it will return all courses) courses where every word of the query is inside the given vocabulary and is sorted by the cosine similarity in descending order. Before retrieving the courses the given query string is being preprocessed.\n",
    "\n",
    "In the following cell we retrieve the 10 courses closest to the given query using the `vocabulary_preprocessed_description.pkl` vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab9be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inverted index from pickle file\n",
    "with open(\"vocabularies/vocabulary_preprocessed_description.pkl\", \"rb\") as file:\n",
    "    vocabulary = pickle.load(file)\n",
    "\n",
    "query = \"advanced knowledge\"\n",
    "engine.retrieve_courses(query, vocabulary, k=10).iloc[:, [0, 1, 2, 3, 9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b152dcc",
   "metadata": {},
   "source": [
    "## 3. Define a new score!\n",
    "Now it's your turn: build a new metric to rank MSc degrees.\n",
    "\n",
    "Practically:\n",
    "\n",
    "1. The user will enter a text query. As a starting point, get the query-related documents by exploiting the search engine of Step 2.1.\n",
    "2. Once you have the documents, you need to sort them according to your new score. In this step, you won't have any more to take into account just the `description` field of the documents; you can use also the remaining variables in your dataset (or new possible variables that you can create from the existing ones or scrape again from the original web-pages). You must use a heap data structure (you can use Python libraries) for maintaining the top-k documents.\n",
    "\n",
    "**N.B.:** You have to define a scoring function, not a filter!\n",
    "\n",
    "The output, must contain:\n",
    "\n",
    "- `courseName`\n",
    "- `universityName`\n",
    "- `description`\n",
    "- `URL`\n",
    "- The **new** similarity score of the documents with respect to the query\n",
    "\n",
    "Are the results you obtain better than with the previous scoring function? **Explain and compare results**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cf0e82",
   "metadata": {},
   "source": [
    "First of all, we made a function **create_query** that takes in input the user's query, then we create the user_query dictionary containing the user's query details.\n",
    "The second function **search_query** uses the created query to retrieve related documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b75bb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_query():\n",
    "    query_terms = input(\"Enter the keywords or terms for your query (separated by space): \")\n",
    "    city = input(\"Enter the city: \")\n",
    "    country = input(\"Enter the country: \")\n",
    "    min_fees = input(\"Enter the minimum fees: \")\n",
    "    max_fees = input(\"Enter the maximum fees: \")\n",
    "\n",
    "    query = {\n",
    "        \"query_terms\": [term.strip() for term in query_terms.split(\" \")],\n",
    "        \"city\": city.strip(),\n",
    "        \"country\": country.strip(),\n",
    "        \"min_fees\": float(min_fees.strip()) if min_fees.strip() else None,\n",
    "        \"max_fees\": float(max_fees.strip()) if max_fees.strip() else None\n",
    "    }\n",
    "\n",
    "    return query\n",
    "\n",
    "def search_query(query, vocabulary):\n",
    "    if not query or not query.get('query_terms'):\n",
    "        return \"Please enter valid query terms.\"\n",
    "\n",
    "    # convert query terms to a string\n",
    "    query_string = ' '.join(query['query_terms'])\n",
    "\n",
    "    # remove query_terms from the dictionary as it's not used in retrieve_courses\n",
    "    query.pop('query_terms', None)\n",
    "\n",
    "    # retrieve the query-related documents using the search engine\n",
    "    result_df = engine.retrieve_courses(query_string, vocabulary, k=15).iloc[:, [0, 1, 2, 3]]\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# use create_query \n",
    "user_query = create_query()\n",
    "\n",
    "# use the user's query to search for related documents\n",
    "search_results = search_query(user_query, vocabulary)\n",
    "print(search_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd4537",
   "metadata": {},
   "source": [
    "Now we define a new scoring function that generetes a final score for each documents that matches with the user's query. The scoring function assigns weights to each criteria we decided: the term in the description, the city, the country and the fees range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ade19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_scoring_function(document, user_query):\n",
    "    \n",
    "    query_terms = user_query.get('query_terms', [])\n",
    "    city_query = user_query.get('city', '')\n",
    "    country_query = user_query.get('country', '')\n",
    "    fee_range = (user_query.get('min_fees', 0), user_query.get('max_fees', float('inf')))\n",
    "    \n",
    "    query_match_score = sum(term in document['description'] for term in query_terms)\n",
    "    city_score = int(city_query in document.get('city', ''))\n",
    "    country_score = int(country_query in document.get('country', ''))\n",
    "    fee_score = int(fee_range[0] <= document.get('fees', 0) <= fee_range[1])\n",
    "        \n",
    "    weights = {\n",
    "        'query_match': 0.6,\n",
    "        'city': 0.1,\n",
    "        'country': 0.2,\n",
    "        'fees': 0.1\n",
    "    }\n",
    "\n",
    "    final_score = (query_match_score * weights['query_match'] +\n",
    "                   city_score * weights['city'] +\n",
    "                   country_score * weights['country'] +\n",
    "                   fee_score * weights['fees'])\n",
    "    \n",
    "    return final_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734e635",
   "metadata": {},
   "source": [
    "The last step we do is to calculate the score with the function we just made and based on this score we extract the top-k documents. As output we can find:\n",
    "- `courseName`\n",
    "- `universityName`\n",
    "- `description`\n",
    "- `URL`\n",
    "- `new score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "# define the value of k for top-k documents\n",
    "k = 15\n",
    "\n",
    "# list to hold scored documents\n",
    "scored_docs = []\n",
    "\n",
    "# iterate through each document in search results\n",
    "for index, document in search_results.iterrows():\n",
    "    # calculate the score using the new scoring function\n",
    "    score = new_scoring_function(document, user_query)\n",
    "    \n",
    "    # add the score to the document details\n",
    "    document_with_score = {\n",
    "        'courseName': document['courseName'],\n",
    "        'universityName': document['universityName'],\n",
    "        'description': document['description'],\n",
    "        'URL': document['url'],\n",
    "        'new_score': score\n",
    "    }\n",
    "    \n",
    "    # append the document with the score to the list\n",
    "    scored_docs.append(document_with_score)\n",
    "\n",
    "# create a heap of top-k documents based on the new similarity score\n",
    "top_k_doc = heapq.nlargest(k, scored_docs, key=lambda x: x['new_score'])\n",
    "\n",
    "# print the top-k documents\n",
    "for doc in top_k_doc:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e10810",
   "metadata": {},
   "source": [
    "Have to comment but the scores are strange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb61e8e",
   "metadata": {},
   "source": [
    "## 4. Visualizing the most relevant MSc degrees\n",
    "Using maps can help people understand how far one university is from another so they can plan their academic careers more adequately. Here, we challenge you to show a map of the courses found with the score defined in point 3. You should be able to identify at least the city and country for each MSc degree. You can find some ideas on how to create maps in Python [here](https://github.com/Sapienza-University-Rome/ADM/tree/master/2023/Homework_3#:~:text=maps%20in%20Python-,here,-and%20here%20but) and [here](https://github.com/Sapienza-University-Rome/ADM/tree/master/2023/Homework_3#:~:text=Python%20here%20and-,here,-but%20you%20will) but you will maybe need further information for a proper visualization, like coordinates (latitude and longitude). You can retrieve this data using various tools:\n",
    "\n",
    "1. [Here](https://github.com/Sapienza-University-Rome/ADM/tree/master/2023/Homework_3#:~:text=using%20various%20tools%3A-,Here,-you%20can%20find) you can find a helpful tutorial on how to encode geo-informations using Google API in Python (this tool can also be used in [Google Sheets](https://github.com/Sapienza-University-Rome/ADM/tree/master/2023/Homework_3#:~:text=be%20used%20in-,Google%20Sheets,-)))\n",
    "2. You can collect a list of unique places in the format (City, Country) and ask chatGPT (or, as usual, any other LLM chatbot) to provide you with a list of corresponding representative coordinates\n",
    "3. Explore and find the best solution for your case!\n",
    "\n",
    "Once you defined your visualization strategy, include a way to encode fees in your charts. The map should show (with a proper legend) different courses and associated taxation: the user wants a glimpse not only of how far he will need to move but also of how much it will cost him!\n",
    "\n",
    "---\n",
    "\n",
    "We decided to use the Google Maps client to retrieve the latitude and longitude given an address. First we retrieve the top 100 courses using our score from point 3. Then we concatenate the columns `universityName`, `city`, and `country` to create our new column `address`. By using this column we retrieve the coordinates of every course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b06207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Google Maps client\n",
    "gmaps = googlemaps.Client(key=\"AIzaSyDSRFQqRgKSlvHeSAEjva_28l-OCEqk21g\")\n",
    "\n",
    "query = \"data science\"\n",
    "\n",
    "#####################################\n",
    "### CHANGE USING THE SCORE FROM 3 ###\n",
    "#####################################\n",
    "\n",
    "courses = engine.retrieve_courses(query, vocabulary, k=100)[[\"courseName\", \"universityName\", \"city\", \"country\", \"fees (‚Ç¨)\"]]\n",
    "\n",
    "courses[\"address\"] = courses[\"universityName\"] + \", \" + courses[\"city\"] + \", \" + courses[\"country\"]\n",
    "\n",
    "courses[\"lat\"] = \"\"\n",
    "courses[\"long\"] = \"\"\n",
    "\n",
    "# Retrive the latitude and longtitude from given addresses\n",
    "for i in range(len(courses)):\n",
    "    geocode_result = gmaps.geocode(courses[\"address\"][i])\n",
    "    courses.loc[i, \"lat\"] = geocode_result[0][\"geometry\"][\"location\"][\"lat\"]\n",
    "    courses.loc[i, \"long\"] = geocode_result[0][\"geometry\"][\"location\"][\"lng\"]\n",
    "\n",
    "courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e95923",
   "metadata": {},
   "source": [
    "Afte we retrieve the top *k* courses we replace every empty value in the field `fees (‚Ç¨)`. In addition, we add a small offset for the longitude and latitude of every coordinate so that points for the same university don't overlap.\n",
    "\n",
    "We decided to use the packages `gpd` and `KeplerGl` to visualize our results with the predefined Kepler config file `kepler_config.json`. The result is being saved as an interactive Kepler map in `map.html`. If you open the file in your browser of choice it will show by default the whole world map. You are able to zoom in and and out. On the right side you are able to enable the legend by clicking on the button `show legend`. The legend shows the mapping of the color of the points to the corresponding range of fees using 10 steps based on the current data of fees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all values where we don't have a fee with 0\n",
    "courses[\"fees (‚Ç¨)\"] = courses[\"fees (‚Ç¨)\"].fillna(0)\n",
    "\n",
    "# Add a random small offset for every data point so points for the same university don't overlap\n",
    "geometry = [Point(xy) for xy in zip(courses[\"long\"] + np.random.normal(-0.005, 0.005, len(courses)),\n",
    "                                    courses[\"lat\"] + np.random.normal(-0.005, 0.005, len(courses)))]\n",
    "\n",
    "# Open kepler config file\n",
    "with open(\"kepler_config.json\", \"r\") as f:\n",
    "    custom_config = json.load(f)\n",
    "\n",
    "# Create a geodataframe with the found courses inside the pandas dataframe\n",
    "gdf = gpd.GeoDataFrame(courses, geometry=geometry)\n",
    "\n",
    "# Create map with kepler\n",
    "map_file = KeplerGl(height=600, width=800, config=custom_config)\n",
    "map_file.add_data(data=gdf, name=\"Visualizing the most relevant MSc degrees\")\n",
    "\n",
    "# Save file as an interactive html file\n",
    "map_file.save_to_html(file_name=\"map.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e3a5e2",
   "metadata": {},
   "source": [
    "## 5. BONUS: More complex search engine\n",
    "For the Bonus part, we want to ask you more sophisticated search engine. Here we want to let users issue more complex queries. The options of this new search engine are:\n",
    "\n",
    "1. Give the possibility to specify queries for the following features (the user should have the option to issue none or all of them):\n",
    "\n",
    "- `courseName`\n",
    "- `universityName`\n",
    "- `city`\n",
    "\n",
    "2. Specify a range for the **fees** to retrieve only MSc whose taxation is in that range.\n",
    "3. Specify a list of **countries** which the search engine should only return the courses taking place in city within those countries.\n",
    "4. Filter based on the courses that have already started.\n",
    "5. Filter based on the presence of online modality.\n",
    "\n",
    "**Note 1:** You should be aware that you should give the user the possibility <ins>to select any</ins> of the abovementioned options. How should the user use the options? We will accept any manual that you provide to the user.\n",
    "\n",
    "**Note 2:** As you may have realized from **1st option**, you need to build <ins>inverted indexes</ins> for those values and return all of the documents that have the similarity <ins>more than 0</ins> concerning the given queries. Choose a logical way to aggregate the similarity coming from each of them and explain your idea in detail.\n",
    "\n",
    "**Note 3:** The options <ins>other than 1st</ins> one can be considered as filtering criteria so the retrieved documents <ins>must respect all</ins> of those filters.\n",
    "\n",
    "The output must contain the following information about the places:\n",
    "\n",
    "- `courseName`\n",
    "- `universityName`\n",
    "- `URL`\n",
    "\n",
    "---\n",
    "\n",
    "First we create the inverted index of the columns `courseName`, `universityName`, and `city` and save the vocabularies in seperated pickle files inside the `vocabularies` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "engine.create_inverted_index2(\"courseName\")\n",
    "engine.create_inverted_index2(\"universityName\")\n",
    "engine.create_inverted_index2(\"city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e80843",
   "metadata": {},
   "source": [
    "We created a function `complex_search_engine` that first lets a user input some parameters to create a query. Based on this query the function returns all courses based on the aggregated similarity between all three inverted indexes and applied filters from the query parameters.\n",
    "\n",
    "Query input:\n",
    "\n",
    "```\n",
    "Enter Course Name (Press Enter to skip): \n",
    "Enter University Name (Press Enter to skip): \n",
    "Enter City (Press Enter to skip): \n",
    "Enter minimum fees in ‚Ç¨ (Press Enter to skip): \n",
    "Enter maximum fees in ‚Ç¨ (Press Enter to skip): \n",
    "Enter a comma-separated list of countries (Press Enter to skip): \n",
    "Filter based on courses that have already started? (y/n): \n",
    "Filter based on the presence of online modality? (y/n): \n",
    "```\n",
    "\n",
    "We decided to use the arithmetic mean to aggregate the cosine similarity between all three vocabularies because for us all three inputs `courseName`, `universityName`, and `city` are equaly important. We were also considering to use the product of all similarities but decided against this aggregation because if one cosine similarity is comparably much smaller than the other two it would have a huge impact on the aggregated similarity which would potentially falsify our result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.complex_search_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e36ede3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
